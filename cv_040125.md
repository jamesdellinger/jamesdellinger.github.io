---
layout: cv
title: "James Dellinger"
permalink: /cv/
---

## Summary
<ul>
<li><a href="#commodities-trading">Commodities trading</a></li>
<li><a href="#decision-trees-library">Decision trees engineering</a></li>
<li><a href="#peel-technologies">Consumer app product manager</a></li>
<li><a href="#sanjie-foreign-language-school">Worked three years in Beijing</a></li>
<li><a href="#fluent-japanese-conversational-korean-and-chinese">JLPT Level 1</a></li>
<li><a href="#bachelor-of-arts-in-economics-uc-berkeley">Econ major</a></li>
</ul>

## Commodities Trading and Decision Trees
<div class="date_city_text"><p class="city_text">Mountain View, CA</p><p>December 2019–Present</p></div>
### Commodities Trading
To supplement my income so that I could complete my decision trees library and book, I learned to trade commodities on a short-term basis. Specifically, I trade a handful of highly liquid markets (e.g. Gold and S&P 500 Index futures) a few times a week. My typical holding times range from a few hours to a couple of days. I follow a strategy that I developed on my own—a systematic and discretionary application of George Douglass Taylor's principles. 

Over a period of two years, I wrote a book called *Trade Like Taylor*, in which I explain his most important concepts in straightforward, reader-friendly language. I published it in early February 2025, and it is available [over at Amazon](https://www.amazon.com/Trade-Like-Taylor-Douglass-Short-Term/dp/B0DWPR9FS9/). 

### Decision Trees Library
Despite having specialized in deep learning, when it came time for me to decide what to build first, I fell in love with decision trees. I saw a need for a Python random forests library that gives practitioners a wide variety of options for splitting categorical variables. The library I'm currently developing will offer the option to use classic CART categorical splits. By default, however, it will employ [Coppersmith, Hong, and Hosking's](https://link.springer.com/article/10.1023/A:1009869804967) more efficient PCA rank encoding preprocessing technique. This is the approach Marvin Wright [uses](https://pmc.ncbi.nlm.nih.gov/articles/PMC6368971/pdf/peerj-07-6339.pdf) in his excellent Ranger library for R. The core and most computationally intensive areas of my package are complete and fully optimized. As a sneak preview, here are two notebooks where I experimented with a variety of sorting and splitting implementations:
- [Sorting experiments](https://github.com/jamesdellinger/recent-work/blob/main/numerical_sorting_speed_experiments.ipynb). David Musser's [introsort](https://www.cs.rpi.edu/~musser/gp/algorithms.html) is fastest, and it doesn't go quadratic when encountering a median-of-3 killer sequence. I discuss Sklearn's short-lived [refactor](https://github.com/scikit-learn/scikit-learn/pull/22868) of its decision tree sorting logic that purported to offer a 15% speed-up. Unfortunately, this was based on a naive benchmark that used a synthetic dataset generated under a normal distribution. Users promptly reported slowness, and the change was soon [reverted](https://github.com/scikit-learn/scikit-learn/pull/23410). This would anecdotally seem to confirm Musser's hunch that median-of-3 killers are more common in practice than should be the case if they merely occurred under a uniform distribution.
- [Numerical splitting experiments](https://github.com/jamesdellinger/recent-work/blob/main/numerical_split_speed_experiments.ipynb). A bake-off between two popular implementations of decision tree numerical splitters. I pitted [Gilles Louppe's](https://glouppe.github.io/) approach (used in Sklearn) against [Marvin Wright's](https://wrig.de/) technique (used in Ranger). A hybrid of the two is fastest, but Louppe's way is best for most cases.

I have also written the first draft a book that will teach students the ins and outs of decision tree algorithms and demonstrate the techniques necessary to make them as performant as possible. Here is [an excerpt](https://github.com/jamesdellinger/recent-work/blob/main/dt_book_excerpt.ipynb).


## Deep Learning Self-Study
<div class="date_city_text"><p>Mountain View, CA</p><p>June 2017–December 2019</p></div>
Conventional wisdom dictates that the best product managers begin their careers as technical ICs, and later on develop an ethos of advocating for users. A similar line of thinking also maintains that the best PMs would otherwise already be founders were they not working at their present companies. After several interviews with members of product orgs at many companies both large and small, I began to have my doubts about both of those assertions. I concluded that for me to continue on the path toward becoming a builder and entrepreneur, I would need to become far more technical than I had ever been.

To that end, I dove head-first into studying statistics, data science, and machine learning via a succession of Udacity nanodegrees. I followed this up by spending one year and 100% of my time going through the entire fast.ai deep learning curriculum with a fine tooth comb. My journey's crescendo was joining with a small and incredibly talented cohort of other full-time students that sat with Jeremy Howard all day, every day during the spring of 2019, while he worked on each week's lecture. Another highlight was our close and consistent interaction with Chris Lattner and the rest of Google's Swift for TensorFlow team. We picked their brains about how they were developing both that library and the [MLIR compiler](https://mlir.llvm.org/). They sought feedback from us on our experiences setting up the library and its usability.

I found that I learned best by coding and then explaining in long-form English what that code does. Here are some noteworthy projects that I completed:
- A [medium post](https://medium.com/data-science/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79) that briefly went viral, in which I presented Jeremy's teaching on weight initialization in a style and pedagogy that made sense to me.
- My [notebook](https://github.com/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/12_text_my_reimplementation.ipynb) explaining how to write the logic to train an English language model and IMDB movie review sentiment classifier *from scratch*.
- I [demonstrated](https://github.com/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/13_swift_resnet_pipeline_s4tf_v04_my_reimplementation.ipynb) the promise, but also analyzed achilles' heels of the Swift for TensorFlow Library.
- I [appeared](https://www.youtube.com/watch?v=4kMEdDcBt00&list=PLLvvXm0q8zUbiNdoIazGzlENMXvZ9bd3x&index=129) on Sanyam Bhutani's Chai Time Data Science podcast to discuss Nvidia's [DALI](https://github.com/NVIDIA/DALI) GPU-accelerated image augmentation library.
- Solo bronze medals in my first two Kaggle competitions:
    1. Home Credit Default Risk [Competition](https://www.kaggle.com/competitions/home-credit-default-risk/overview). I used a gradient boosted trees model, along with various pre and post-processing bells and whistles, to predict which loan applicants would default. [Solution summary](https://www.kaggle.com/competitions/home-credit-default-risk/discussion/64890).
    2. TGS Salt Identification [Challenge](https://www.kaggle.com/competitions/tgs-salt-identification-challenge/overview). I trained a U-Net neural network to segment images for salt deposits. [Solution summary](https://www.kaggle.com/competitions/tgs-salt-identification-challenge/discussion/69136).


## Peel Technologies
<div class="date_city_text"><p>Mountain View, CA</p><p>May 2014–December 2016</p></div>
Peel's TV guide and remote control app was preloaded worldwide on virtually all Android smart phones that included a built-in IR blaster during the mid-2010s. At its peak it had DAUs in the millions.
### Product Evangelist
Directly assisted the Co-founder/CEO in a variety of vital projects ranging from preparing decks for his speeches, to office management, scheduling, and marketing. I was blessed with the opportunity to observe firsthand what it took to run and grow a successful startup that had just begun to gain traction. My chief responsibility was creating and then performing product demos and pitches for potential investors and partners. This required close and continuing collaboration with a couple of our developers.
### Product Manager
After one year, I began spending the bulk of my time managing our app's setup funnel. It was incredibly challenging to communicate our app's purpose to brand new users and make them able to successfully control their TV and set-top box as quickly as possible, even as doing so required line of sight communication between their phones and these devices. A small team of me, one of our Android engineers, and a few UX/UI designers were, after much user research, A/B testing, and steady iteration, able to 2x the fraction of new users who successfully tuned into TV shows from their phones.

I also worked on improving the settings screen, where users could configure specialty devices like AV Receivers. This was an unglamorous but crucial part of our app, as users who added more devices tended to keep using the app. I tend to think in a logical, systematic, and organized manner, and this helped me to design flows that were as simple as possible, but could still accommodate our power users' most complex home theater setups.

## Google Ads Quality
<div class="date_city_text"><p>Mountain View, CA</p><p>December 2012–December 2013</p></div>
My introduction to applied machine learning was, ironically enough, serving among an army of contractors who ran sanity checks on the ad placement algorithms that Google employed at this time.

## Sanjie Foreign Language School
<div class="date_city_text"><p>Beijing, China</p><p>September 2008–August 2011</p></div>
English teacher for youngsters from South Korea who boarded at private a school located in the Wangjing neighborhood of Beijing's Chaoyang district. I also occasionally taught high school math and SAT prep. It's debatable whether I taught my students more English or they taught me more Korean. Either way, it was a personally rewarding multi-year journey with the same cohort of students as they prepared to eventually attend university in Beijing.

## Cafe Chewhadang
<div class="date_city_text"><p>Seoul, Korea</p><p>April–September 2007</p></div>
Co-managed a small book cafe near Yonsei University's east gate. This was my introduction to the finer points of the daily operation of a small business, including procurement, quality control, bookkeeping, and sales and outreach (being on the receiving end of bemused chuckling from students at the nearby Ewha Women's College).

## Bachelor of Arts in Economics, UC Berkeley
My favorite classes were Econometrics with [Professor Paul Ruud](https://eml.berkeley.edu/econ/faculty/ruud_p.shtml), Chinese Modern Economic History with [Professor Yingyi Qian](https://www.sem.tsinghua.edu.cn/en/info/1215/5722.htm), and Psychology and Economics with [Professor Botond Koszegi](https://en.wikipedia.org/wiki/Botond_K%C5%91szegi). Despite its name, Psych and Econ was highly technical, as we covered techniques like hyperbolic discounting. Perhaps my proudest academic achievement was simply finishing near the top of [Professor Jeffrey Perloff's](https://are.berkeley.edu/users/jeffrey-perloff) microeconomics weeder class (Econ 100A)—not a small feat given that Econ was an incredibly in-demand major at Cal.

## Fluent Japanese, conversational Korean and Chinese
I passed the JLPT Level 1 in December 2006. This test certifies that one's Japanese is proficient enough for university-level study in Japan.

## Hobby: restoring old circuit boards
After months of painstaking cleaning and soldering, I brought a vintage PC Engine system back to life during the summer of 2020. Here is [a chronicle](https://github.com/jamesdellinger/recent-work/blob/main/duo.ipynb) of the process.

## Contact
<ul>
<li>First name dot last name at gmail dot com.</li>
<li><a href="https://www.kaggle.com/jamesdellinger">Kaggle profile</a></li>
<li><a href="https://github.com/jamesdellinger">Github</a></li>
</ul>